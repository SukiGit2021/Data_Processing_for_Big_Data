{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STUDENT ID: 31153291\n",
    "#### STUDENT NAME: PEIYU LIU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents<a class=\"anchor\" id=\"table\"></a>\n",
    "\n",
    "* [1 Working with RDD](#1)\n",
    "* [1.1 Data Preparation and Loading](#1.1)\n",
    "* [1.1.1 Creating SparkSession & SparkContext](#OneOneOne)\n",
    "* [1.1.2 Read CSV files, Preprocessing, and final(formatted data) RDD for each file](#OneOneTwo)\n",
    "* [1.1.2.1 Flights RDD](#1.1.2.1)\n",
    "* [1.1.2.2 Airports RDD](#1.1.2.2)\n",
    "* [1.1.3 Show RDD number of columns, and number of records](#1.1.3)\n",
    "* [1.2 Dataset flights partitioning](#1.2)\n",
    "* [1.2.1 Obtain the maximum arrival time ](#1.2.1)\n",
    "* [1.2.2 Obtain the maximum minimum time ](#1.2.2)\n",
    "* [1.2.3 Define hash partitioning](#1.2.3)\n",
    "* [1.2.4 Display the records in each partition](#1.2.4)\n",
    "* [1.3 Query RDD](#1.3)\n",
    "* [1.3.1 Collect a total number of flights for each month for all flights](#1.3.1)\n",
    "* [1.3.2 Collect the average delay for each month for all flights](#1.3.2)\n",
    "* [2 Working with DataFrames](#2)\n",
    "* [2.1 Data Preparation and Loading](#2.1)\n",
    "* [2.1.1 Define DataFrames](#2.1.1)\n",
    "* [2.1.2 Display the Scheme of DataFrames](#2.1.2)\n",
    "* [2.1.3 Transform date-time and location column](#2.1.3)\n",
    "* [2.2.1 January Flights Events with ANC airport](#2.2.1)\n",
    "* [2.2.2 Average Arrival Delay From Origin to Destination](#2.2.2)\n",
    "* [2.2.3 Join Query with Airports DataFrame](#2.2.3)\n",
    "* [2.3 Analysis](#2.3.1)\n",
    "* [2.3.1 Relationship between day of week with mean arrival delay, total time delay, and count flights](#2.3.1)\n",
    "* [2.3.2 Display mean arrival delay each month](#2.3.2)\n",
    "* [2.3.3 Relationship between mean departure delay and mean arrival delay](#2.3.3)\n",
    "* [3 RDDs vs DataFrame vs Spark SQL](#3)\n",
    "* [3.1 RDD Operation](#3.1)\n",
    "* [3.2 DataFrame Operation](#3.1)\n",
    "* [3.3 Spark SQL Operation](#3.1)\n",
    "* [3.4 Discussion](#3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Working with RDD<a class=\"anchor\" id=\"1\"></a>\n",
    "## 1.1 Data Preparation and Loading<a class=\"anchor\" id=\"1.1\"></a>\n",
    "### 1.1.1 Create SparkSession and SparkContext<a class=\"anchor\" id=\"OneOneOne\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries  that assignment nended.\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql.types import IntegerType\n",
    "# do file literated \n",
    "import os,sys\n",
    "# do pyspark sql process\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#create a SparkContext object using SparkSession\n",
    "master = 'local[*]'\n",
    "app_name = '31153291_Ass1'\n",
    "#build a SparkConf\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Import CSV files and Make RDD for each file<a class=\"anchor\" id=\"OneOneTwo\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** list all files:https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "\n",
    "Literate all flight files in flight folder.\n",
    "Add files' names with files' path into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "file_path = './flight-delays/'\n",
    "files = os.listdir(file_path)\n",
    "file_container = []\n",
    "for file in files:\n",
    "    if file.startswith('flight'): # only need flights*files\n",
    "        print(file)\n",
    "        file_container.append('./flight-delays/'+file )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** string content join: https://www.w3schools.com/python/ref_string_join.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all data from raw csv files\n",
    "airports_raw_data = sc.textFile('./flight-delays/airports.csv')\n",
    "# combine each flight data into one aggregate and divide by ','\n",
    "flights_raw_data = sc.textFile(','.join(file_container))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.1 Flights RDD <a class=\"anchor\" id=\"1.1.2.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- remove file headers\n",
    "- rdd.first(): read first row\n",
    "- lambda loop all rows\n",
    "- filter to filtrate all rows that equal to header row \n",
    "- split header into multiple values\n",
    "- convert 'YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'FLIGHT_NUMBER' into integer format\n",
    "- convert 'DEPARTURE_DELAY', 'ARRIVAL_DELAY', 'ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'TAXI_OUT', 'TAXI_IN' into float format\n",
    "- for loop to check each columns index in header\n",
    "- after get all index of columns that I need to conver format use index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove file headers\n",
    "# rdd.first(): read first row\n",
    "aps_header = airports_raw_data.first()\n",
    "fls_header = flights_raw_data.first()\n",
    "\n",
    "# lambda loop all rows\n",
    "# filter to filtrate all rows that equal to header row \n",
    "airports_rdd = airports_raw_data.filter(lambda flag: flag != aps_header)\n",
    "flights_rdd = flights_raw_data.filter(lambda flag: flag != fls_header)\n",
    "\n",
    "# split header into multiple values.\n",
    "flight_header_list = fls_header.split(',')\n",
    "airport_header_list = aps_header.split(',')\n",
    "\n",
    "# convert 'YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'FLIGHT_NUMBER' into integer format.\n",
    "int_fields = ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'FLIGHT_NUMBER']\n",
    "# convert 'DEPARTURE_DELAY', 'ARRIVAL_DELAY', 'ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', \n",
    "# 'TAXI_OUT', 'TAXI_IN' into float format.\n",
    "float_fields = ['DEPARTURE_DELAY', 'ARRIVAL_DELAY', 'ELAPSED_TIME', 'AIR_TIME',\n",
    "                'DISTANCE','TAXI_OUT', 'TAXI_IN']\n",
    "\n",
    "# for loop to check each columns index in header.\n",
    "int_container = []\n",
    "for flag1,flag2 in enumerate(flight_header_list):\n",
    "    if flag2 in int_fields:\n",
    "        int_container.append(flag1)\n",
    "        \n",
    "print(int_container)\n",
    "print('*************************')\n",
    "\n",
    "# for loop to check each columns index in header.\n",
    "float_container = []\n",
    "for flag1, flag2 in enumerate(flight_header_list):\n",
    "    if flag2 in float_fields:\n",
    "        float_container.append(flag1)\n",
    "        \n",
    "print(float_container)\n",
    "\n",
    "# check index of LATITUDE and LONGITUDE.\n",
    "airport_container = []\n",
    "for flag1,flag2 in enumerate(airport_header_list):\n",
    "    if flag2 == \"LATITUDE\":\n",
    "        print(\"LATITUDE:\"+str(flag1))\n",
    "    else:\n",
    "        if flag2 == \"LONGITUDE\":\n",
    "            print(\"LONGITUDE:\"+str(flag1))\n",
    "                    \n",
    "# after get all index of columns that I need to conver format, then do convert actions below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** \n",
    "\n",
    "- convert rows to dictionary row:https://stackoverflow.com/questions/49432167/how-to-convert-rows-into-a-list-of-dictionaries-in-pyspark\n",
    "- change None value: https://stackoverflow.com/questions/45489357/change-none-to-float-in-list-of-strings-python\n",
    "- python dict zip: https://www.codegrepper.com/code-examples/python/python+dict+zip\n",
    "- Row actions:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Row.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: \n",
    "# convert rows to dictionary row:\n",
    "#https://stackoverflow.com/questions/49432167/how-to-convert-rows-into-a-list-of-dictionaries-in-pyspark\n",
    "# Use rdd Row function to convert data into Row format{column name: value}\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# fucntion to split data by ',' into each row.\n",
    "# literate all values in each row to check None value,\n",
    "# None value loop will result in breakdown, so that change None value into readable format.\n",
    "# Reference :\n",
    "# change None value: \n",
    "#https://stackoverflow.com/questions/45489357/change-none-to-float-in-list-of-strings-python\n",
    "def flightsRddConvertRow(rddRow):\n",
    "    rowEach = rddRow.split(',')\n",
    "    for flag1, flag2 in enumerate(rowEach):\n",
    "        if flag1 in int_container:\n",
    "            if flag2 == '':\n",
    "                rowEach[flag1] = float('nan')\n",
    "            else:\n",
    "                rowEach[flag1] = int(flag2)\n",
    "        if flag1 in float_container:\n",
    "            if flag2 == '':\n",
    "                rowEach[flag1] = float('nan')\n",
    "            else:\n",
    "                rowEach[flag1] = float(flag2)\n",
    "                \n",
    "# Reference: python dict zip: https://www.codegrepper.com/code-examples/python/python+dict+zip\n",
    "# Zip key and value to dictionary and then use **dict to split dictionary, \n",
    "# Use Row function to create a row object by using named arguments and values.\n",
    "# Reference: Row actions:\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Row.html\n",
    "    return Row(**dict(zip(flight_header_list, rowEach)))\n",
    "\n",
    "new_flights_rdd_row = flights_rdd.map(flightsRddConvertRow)\n",
    "new_flights_rdd_row.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.2 Airports RDD <a class=\"anchor\" id=\"1.1.2.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** \n",
    "\n",
    "convert rows to dictionary row:\n",
    "- https://stackoverflow.com/questions/49432167/how-to-convert-rows-into-a-list-of-dictionaries-in-pyspark\n",
    "\n",
    "change None value: \n",
    "- https://stackoverflow.com/questions/45489357/change-none-to-float-in-list-of-strings-python\n",
    "\n",
    "python dict zip: \n",
    "- https://www.codegrepper.com/code-examples/python/python+dict+zip\n",
    "\n",
    "Row actions:\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Row.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fucntion to split data by ',' into each row.\n",
    "# literate all values in each row to check None value,\n",
    "# None value loop will result in breakdown, so that change None value into readable format.\n",
    "# Reference : \n",
    "# change None value: \n",
    "# https://stackoverflow.com/questions/45489357/change-none-to-float-in-list-of-strings-python\n",
    "def airportsRddConvertRow(rddRow):\n",
    "    rowEach = rddRow.split(',')\n",
    "    for flag1, flag2 in enumerate(rowEach):\n",
    "        if flag1 in [5, 6]:  # latitude, langitude\n",
    "            if flag2 == '':\n",
    "                rowEach[flag1] = float('nan')\n",
    "            else:\n",
    "                rowEach[flag1] = float(flag2)\n",
    "\n",
    "# Reference: python dict zip: https://www.codegrepper.com/code-examples/python/python+dict+zip\n",
    "# Zip key and value to dictionary and then use **dict to split dictionary, \n",
    "# Use Row function to create a row object by using named arguments and values.\n",
    "# Reference: Row actions:\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Row.html\n",
    "    return Row(**dict(zip(airport_header_list, rowEach)))\n",
    "\n",
    "\n",
    "new_airports_rdd_row = airports_rdd.map(airportsRddConvertRow)\n",
    "print(new_airports_rdd_row.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Show RDD number of columns, and number of records <a class=\"anchor\" id=\"1.1.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** \n",
    "- partitioning function:https://kontext.tech/column/spark/299/data-partitioning-functions-in-spark-pyspark-explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_partitions(df):\n",
    "    numPartitions = df.getNumPartitions()\n",
    "    print(f\"Number of partitions:{numPartitions}\")\n",
    "    partitions = df.glom().collect()\n",
    "    for index,partition in enumerate (partitions):\n",
    "        if len(partition)>0:\n",
    "            print(f\"Partition{index}: {len(partition)} records\")\n",
    "            \n",
    "# flights records\n",
    "print('Flights count:'+str(new_flights_rdd_row.count()))\n",
    "print_partitions(new_flights_rdd_row)\n",
    "print('--------------------------------------------------')\n",
    "# airposts records\n",
    "print('Airposts count:'+str(new_airports_rdd_row.count()))\n",
    "print_partitions(new_airports_rdd_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dataset Partitioning <a class=\"anchor\" id=\"1.2\"></a>\n",
    "### 1.2.1 Obtain the maximum arrival time <a class=\"anchor\" id=\"1.2.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** RDD.max():http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.max.html\n",
    "\n",
    "lambda to literate all ARRIVAL_DELAY value and use max function to calculate maximum value and filter out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxArrivalDelay = new_flights_rdd_row.filter(lambda x: x['ARRIVAL_DELAY']).max(\n",
    "    key=lambda x: x['ARRIVAL_DELAY'])\n",
    "maxArrivalDelay['ARRIVAL_DELAY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Obtain the minimum arrival time <a class=\"anchor\" id=\"1.2.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** \n",
    "- RDD.min():http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.min.html?highlight=rdd%20min#pyspark.RDD.min\n",
    "\n",
    "lambda to literate all ARRIVAL_DELAY value and use min function to calculate minimum value and filter out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minArrivalDelay = new_flights_rdd_row.filter(lambda x:x['ARRIVAL_DELAY']).min(\n",
    "    key = lambda x:x['ARRIVAL_DELAY'])\n",
    "minArrivalDelay['ARRIVAL_DELAY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Define hash partitioning function <a class=\"anchor\" id=\"1.2.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** repartition: https://www.qedev.com/bigdata/104082.html\n",
    "- HashPartitioner partitions by calculating the hashCode for a given key. \n",
    "- Then use partitionBy to repartition the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash partition function\n",
    "def hash_function(key):\n",
    "    keyCount = 0\n",
    "    for flag in key:\n",
    "        keyCount += int(flag)\n",
    "    return keyCount\n",
    "\n",
    "# hash partitioning: use ARRIVAL_TIME as code.\n",
    "key = new_flights_rdd_row.filter(lambda x: x['ARRIVAL_TIME'] != '').map(lambda x: (x['ARRIVAL_TIME'], x))\n",
    "\n",
    "# Repartition 4\n",
    "newHashRepartition = key.partitionBy(4, hash_function)\n",
    "newHashRepartition.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Display the records in each partition <a class=\"anchor\" id=\"1.2.4\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# records calculator that I designed before.\n",
    "# records in each partition\n",
    "print_partitions(newHashRepartition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Query RDD  <a class=\"anchor\" id=\"1.3\"></a>\n",
    "### 1.3.1 Collect a total number of flights for each month <a class=\"anchor\" id=\"1.3.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** GroupBy key and use collect to count numbers: \n",
    "- https://stackoverflow.com/questions/56895694/group-by-key-value-pyspark\n",
    "\n",
    "- map function to reformat data as (Month,value) format.\n",
    "- use groupby to analyse each month and mapvalue to records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value = new_flights_rdd_row.map(lambda x: (x['MONTH'], x))\n",
    "key_value = key_value.groupByKey().mapValues(lambda x: len(x))\n",
    "month_records = key_value.collect()\n",
    "\n",
    "for key, value in month_records:\n",
    "    print('Month:{0}--Numbers:{1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Collect the average delay for each month <a class=\"anchor\" id=\"1.3.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** PySpark reduceByKey():\n",
    "- merge the values of each key https://sparkbyexamples.com/pyspark/pyspark-reducebykey-usage-with-examples/\n",
    "- The average delay for each month:\n",
    "    - filtrate none value\n",
    "    - filtrate negative value, only calculate delay time\n",
    "    - reformat data to {Month:delay_time, times}\n",
    "    - calculate total delay time and times\n",
    "    - time divide times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgDelayMonth = new_flights_rdd_row.filter(\n",
    "    lambda flag: flag.ARRIVAL_DELAY != None).filter(\n",
    "    lambda row: row.ARRIVAL_DELAY > 0).map(\n",
    "    lambda flag: (flag.MONTH, [flag.ARRIVAL_DELAY, 1])).reduceByKey(\n",
    "    lambda key, key_: [key[0] + key_[0], key[1] + key_[1]]).mapValues(\n",
    "    lambda flag: flag[0] / flag[1]).collect()\n",
    "\n",
    "AvgDelayMonth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Working with DataFrame <a class=\"anchor\" id=\"2\"></a>\n",
    "## 2.1. Data Preparation and Loading <a class=\"anchor\" id=\"2.1\"></a>\n",
    "### 2.1.1 Define dataframes and loading scheme<a class=\"anchor\" id=\"2.1.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the module spark.read.format(“csv”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportsDf = spark.read.format('csv').option('header', True).option('inferSchema', True).load(\n",
    "    'flight-delays/airports.csv')\n",
    "\n",
    "flightsDf = spark.read.format('csv').option('header', True).option('inferSchema', True).load(\n",
    "    'flight-delays/flight*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Display the schema of the final two dataframes<a class=\"anchor\" id=\"2.1.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportsDf.printSchema()\n",
    "flightsDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Query Analysis <a class=\"anchor\" id=\"2.2\"></a>\n",
    "### 2.2.1 January flight events with ANC airport <a class=\"anchor\" id=\"2.2.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** spark.sql functions:https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html\n",
    "\n",
    "- Filter to filtrate data as conditions,\n",
    "- F.col select column name and set conditions.\n",
    "- Use select to set result columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "janFlightEventsAncDf = flightsDf.filter(F.col('YEAR') == 2015).filter(F.col('MONTH') == 1).filter(\n",
    "    F.col('ORIGIN_AIRPORT') == 'ANC').select(\n",
    "    'MONTH', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DISTANCE','ARRIVAL_DELAY')\n",
    "\n",
    "janFlightEventsAncDf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Average Arrival Delay From Origin to Destination <a class=\"anchor\" id=\"2.2.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** aggreate actions:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html\n",
    "\n",
    "- groupBy use conditions\n",
    "- calculate average values and give a new name\n",
    "- sort by ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "janFlightEventsAncAvgDf = janFlightEventsAncDf.groupBy('ORIGIN_AIRPORT', 'DESTINATION_AIRPORT').agg(\n",
    "    F.mean('ARRIVAL_DELAY').alias('AVERAGE_DELAY')).sort('AVERAGE_DELAY', ascending=True)\n",
    "\n",
    "janFlightEventsAncAvgDf.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Join Query with Airports DataFrame <a class=\"anchor\" id=\"2.2.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** \n",
    "- default inner join:http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.join.html?highlight=inner%20join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedSqlDf = airportsDf.join(janFlightEventsAncAvgDf, airportsDf.IATA_CODE \n",
    "                              == janFlightEventsAncAvgDf.ORIGIN_AIRPORT)\n",
    "\n",
    "joinedSqlDf.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Analysis <a class=\"anchor\" id=\"2.3\"></a>\n",
    "### 2.3.1 Relationship between day of week with mean arrival delay, total time delay, and count flights <a class=\"anchor\" id=\"2.3.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ed clarification:https://edstem.org/au/courses/6038/discussion/565373\n",
    "- Total time delay is related to sum of arrival delay,\n",
    "- numOfFlights is related to the number of flights,\n",
    "- average of arrival delay is related to the mean of the arrival delay\n",
    "- group by 'DAY_OF_WEEK'\n",
    "- sort descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayOfWeekDelayDf = flightsDf.filter(\n",
    "    F.col('YEAR') == 2015).withColumn('ARRIVAL_DELAY', F.col('ARRIVAL_DELAY')).groupby('DAY_OF_WEEK').agg(\n",
    "    F.mean('ARRIVAL_DELAY').alias('MeanArrivalDelay'), F.sum('ARRIVAL_DELAY').alias('TotalTimeDelay'),\n",
    "    F.count('FLIGHT_NUMBER').alias('NumOfFlights')).sort('NumOfFlights', ascending=False)\n",
    "\n",
    "dayOfWeekDelayDf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What can you analyse from this query results?\n",
    "Answer: From query results, it can be seen that the largest average delayed arrival time is Friday, and the smallest is Saturday. \n",
    "It can also be seen that the specific value of the average delay time per day during the week and the specific value of the total delay time. \n",
    "It can be seen that when the number of flights is large, the total delay time will be higher than when the number of flights is small. Quantitative changes cause qualitative changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Display mean arrival delay each month <a class=\"anchor\" id=\"2.3.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:**\n",
    "- withcolumn:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html\n",
    "- groupBy:https://hendra-herviawan.github.io/pyspark-groupby-and-aggregate-functions.html\n",
    "- select columns and group\n",
    "- aggregate columns with function processing\n",
    "- create a new column with processing\n",
    "- count times\n",
    "- sort ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthDelayDf = flightsDf.withColumn('ARRIVAL_DELAY', F.col('ARRIVAL_DELAY')).groupby('MONTH').agg(\n",
    "    F.mean('ARRIVAL_DELAY').alias('MeanArrivalDelay'), F.sum('ARRIVAL_DELAY').alias('TotalTimeDelay'),\n",
    "    F.count('FLIGHT_NUMBER').alias('NumOfFlights')).sort('MeanArrivalDelay', ascending=True)\n",
    "\n",
    "monthDelayDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What can you analyse from this query results?\n",
    "Answer: From query results, you can see that the average delay time will have a negative value, and the aircraft may arrive early, so a negative value is normal. September and October have the least delays, and the number of flights per month is similar, and the difference is not very large. The most delayed flight is in June, so the total delay is longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Relationship between mean departure delay and mean arrival delay <a class=\"anchor\" id=\"2.3.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- divide by Month for groups\n",
    "- Aggregate Function to calculate average value in specified columns.\n",
    "- sort by average departure delay value, descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DepArrDelayDf = flightsDf.groupBy('MONTH').agg(\n",
    "    F.mean('DEPARTURE_DELAY').alias('MeanDeptDelay'),\n",
    "    F.mean('ARRIVAL_DELAY').alias('MeanArrivalDelay')).sort('MeanDeptDelay', ascending=False)\n",
    "\n",
    "DepArrDelayDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What you can analyse from the relationship between two columns: Mean Departure Delay and Mean Arrival Delay?\n",
    "Answer: From query results, you can see that the average delay time for departure and the average delay time for arrival are related. When the delay time for take-off is more, the delay time for arrival is more. Late departure and late arrival are related, and the latter is more likely to occur when the former occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 RDDs vs DataFrame vs Spark SQL <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "\n",
    "Implement the following queries using RDDs, DataFrames and SparkSQL separately. Log the time taken for each query in each approach using the “%%time” built-in magic command in Jupyter Notebook and discuss the performance difference of these 3 approaches.\n",
    "\n",
    "<strong>Find the MONTH and DAY_OF_WEEK, number of flights, and average delay where TAIL_NUMBER = ‘N407AS’. Note number of flights and average delay should be aggregated separately. The average delay should be grouped by both MONTH and DAYS_OF_WEEK.</strong>\n",
    "\n",
    "## 3.1 RDD Operation<a class=\"anchor\" id=\"3.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** \n",
    "- PySpark reduceByKey():merge the values of each key https://sparkbyexamples.com/pyspark/pyspark-reducebykey-usage-with-examples/\n",
    "- mapValue collect:https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.mapValues.html\n",
    "- select rows that TAIL_NUMBER is equal 'N407AS'\n",
    "- filtrate none value\n",
    "- change value format as {month,day,departureDelayTime,arrivalDelayTime}\n",
    "- change value type {month,day,departureDelayTime,arrivalDelayTime} as string.\n",
    "- change data value by a+b\n",
    "- calculate average delay time and times\n",
    "- time divide times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_flights_rdd_row.filter(lambda flag: flag.TAIL_NUMBER == 'N407AS').filter(\n",
    "    lambda flag: flag.ARRIVAL_DELAY != \"\").filter(\n",
    "    lambda flag: flag.DEPARTURE_DELAY != \"\").map(\n",
    "    lambda flag: [flag.MONTH, flag.DAY_OF_WEEK, flag.DEPARTURE_DELAY, flag.ARRIVAL_DELAY]).map(\n",
    "    lambda flag: (str([flag[0], flag[1]]), [1, flag[2], flag[3]])).reduceByKey(\n",
    "    lambda k, k_: [k[0] + k_[0], k[1] + k_[1], k[2] + k_[2]]).mapValues(\n",
    "    lambda flag: [flag[0], flag[1] / flag[0], flag[2] / flag[0]]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DataFrame Operation<a class=\"anchor\" id=\"3.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- select rows that TAIL_NUMBER is equal 'N407AS'\n",
    "- group data by 'MONTH', 'DAY_OF_WEEK', 'TAIL_NUMBER'\n",
    "- aggregate functions to calculate average value and assign a new column \n",
    "- count how many fight events\n",
    "- sort by ascending order\n",
    "- generate pandas view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "flightsDf.filter(F.col('TAIL_NUMBER') == 'N407AS').groupBy('MONTH', 'DAY_OF_WEEK', 'TAIL_NUMBER').agg(\n",
    "    F.mean('DEPARTURE_DELAY').alias('AvgDepDelay'), F.mean('ARRIVAL_DELAY').alias('AvgArrDelay'),\n",
    "    F.count('FLIGHT_NUMBER').alias('NumOfFlights')).sort(['MONTH'], ascending=True).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Spark SQL OPERATION<a class=\"anchor\" id=\"3.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.createOrReplaceTempView.html\n",
    "# Creates or replaces a local temporary view with this DataFrame.\n",
    "flightsDf.createOrReplaceTempView(\"flightsSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "spark.sql('''\n",
    "SELECT MONTH,DAY_OF_WEEK,TAIL_NUMBER,\n",
    "MEAN(DEPARTURE_DELAY) AS AvgDepDelay,\n",
    "MEAN(ARRIVAL_DELAY) AS AvgArrDelay,\n",
    "COUNT(FLIGHT_NUMBER) AS NumOfFlights\n",
    "FROM\n",
    "(\n",
    "SELECT MONTH,DAY_OF_WEEK,FLIGHT_NUMBER,DEPARTURE_DELAY,ARRIVAL_DELAY,TAIL_NUMBER\n",
    "FROM flightsSQL\n",
    "WHERE TAIL_NUMBER = 'N407AS'\n",
    ")\n",
    "GROUP BY MONTH,DAY_OF_WEEK,TAIL_NUMBER\n",
    "ORDER BY MONTH\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Discussion<a class=\"anchor\" id=\"3.4\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** https://ipython.readthedocs.io/en/stable/interactive/magics.html?highlight=%25time#magic-time\n",
    "- The CPU time is divided into user and sys. \n",
    "- The user value represents the time used by the program itself and the subroutines in the library it calls. \n",
    "- sys is the execution time of system calls called directly or indirectly by the program.\n",
    "- The wall time from the start of the program to the end of the program execution, including the time used by the CPU.\n",
    "\n",
    "#### RDD Operation:\n",
    "Time:\n",
    "- CPU times: user 22.5 ms, \n",
    "- sys: 10.8 ms, \n",
    "- total: 33.3 ms,\n",
    "- Wall time: 8.31 s\n",
    "\n",
    "#### DataFrame Operation:\n",
    "Time: \n",
    "- CPU times: user 21.3 ms, \n",
    "- sys: 4.83 ms, \n",
    "- total: 26.2 ms,\n",
    "- Wall time: 2.12 s\n",
    "#### SQL Operation:\n",
    "Time: \n",
    "- CPU times: user 16.5 ms, \n",
    "- sys: 3.79 ms, \n",
    "- total: 20.3 ms,\n",
    "- Wall time: 2.47 s\n",
    "\n",
    "When using SQL Operation for data processing, the CPU usage time is the shortest, only 16.5ms. <p>The execution time of system is also the shortest, only 3.79ms. <p>But the shortest time between program start and end of operation is DataFrame Operation.\n",
    "Through time comparison of the three, the most efficient way to process the data this time should be DataFrame Operation. <p>Because the operation needs to select and filter the columns of the data, and then process them to find the result. <p>DataFrame is a distributed data set organized in named columns, which is equivalent to an optimized table in a relational database. <p>DataFrame is more efficient than RDD because it specifies a specific structure to constrain the data. And rdd occupies too much memory, so DataFrame Operation is more suitable for data processing under this condition."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5202 Assignment 1 SOLUTION.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
